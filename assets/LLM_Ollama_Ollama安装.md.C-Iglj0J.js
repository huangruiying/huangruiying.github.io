import{_ as l,c as e,o as s,a1 as t}from"./chunks/framework.CPsxJxQM.js";const c=JSON.parse('{"title":"Ollama","description":"","frontmatter":{},"headers":[],"relativePath":"LLM/Ollama/Ollama安装.md","filePath":"LLM/Ollama/Ollama安装.md"}'),o={name:"LLM/Ollama/Ollama安装.md"};function n(r,a,i,p,h,m){return s(),e("div",null,a[0]||(a[0]=[t(`<h1 id="ollama" tabindex="-1"><a href="https://ollama.com/" target="_blank" rel="noreferrer">Ollama</a> <a class="header-anchor" href="#ollama" aria-label="Permalink to &quot;[Ollama](https://ollama.com/)&quot;">​</a></h1><p>是一个开源的，允许用在本地pc机上基于CPU部署和训练的大模型部署和代理平台。对外会提供可编程接口用于与Ollama平台部署的模型进行交互。</p><h2 id="官网下载ollama" tabindex="-1"><a href="https://ollama.com/download" target="_blank" rel="noreferrer">官网下载Ollama</a> <a class="header-anchor" href="#官网下载ollama" aria-label="Permalink to &quot;[官网下载Ollama](https://ollama.com/download)&quot;">​</a></h2><p>支持MacOS，Linux，Windows系统。</p><h2 id="下载自选模型" tabindex="-1">下载自选模型 <a class="header-anchor" href="#下载自选模型" aria-label="Permalink to &quot;下载自选模型&quot;">​</a></h2><p>可以在<a href="https://ollama.com/search" target="_blank" rel="noreferrer">此处</a> 查找想要下载的模型，拿llama3.3举例 想要下载llama3.3模型，仅需在安装完Ollama后，执行<code>ollama pull llama3.3</code> 即可将模型下载到本地。</p><h2 id="运行模型" tabindex="-1">运行模型 <a class="header-anchor" href="#运行模型" aria-label="Permalink to &quot;运行模型&quot;">​</a></h2><p>当存在模型时会直接启动。当不存在模型时，会自动下载，并运行。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3.3</span></span></code></pre></div><p>安装完成后，就可以通过控制台与大模型进行交互啦。</p><h2 id="模型存储位置" tabindex="-1">模型存储位置 <a class="header-anchor" href="#模型存储位置" aria-label="Permalink to &quot;模型存储位置&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Mac: ~/.ollama/models/</span></span>
<span class="line"><span>Linux（或 WSL）: /usr/share/ollama/.ollama/models</span></span>
<span class="line"><span>Windows: C:\\Users\\Administrator\\.ollama\\models</span></span></code></pre></div>`,12)]))}const u=l(o,[["render",n]]);export{c as __pageData,u as default};
