<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>漏桶 on 博客</title>
    <link>https://huangruiying.github.io/tags/%E6%BC%8F%E6%A1%B6/</link>
    <description>Recent content in 漏桶 on 博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 19 Jul 2023 16:11:18 +0800</lastBuildDate><atom:link href="https://huangruiying.github.io/tags/%E6%BC%8F%E6%A1%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>限流算法之漏桶</title>
      <link>https://huangruiying.github.io/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/%E6%BC%8F%E6%A1%B6%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 19 Jul 2023 16:11:18 +0800</pubDate>
      
      <guid>https://huangruiying.github.io/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/%E6%BC%8F%E6%A1%B6%E6%A8%A1%E5%9E%8B/</guid>
      <description> 漏桶核心流程 通过代码控制请求，通过平均时间段，均匀散布一秒内的请求。
弊端：保证不了每个请求之间正好是100ms，可能会是120ms .. 这样会降低一秒内的请求数。
1.假设1s限流100 【每秒请求量为100，preRequest=1/100】 2.那么漏桶模型则是将100个请求散布在这1s内【请求1处理完成时刻记做limiter.last,请求2进入时刻与last对比，没有达到preRequest,则让当前线程sleep至间隔时间刚好等于preRequest即可】 3.理论上每个请求10ms【正常情况下】 4.但是会出现此种情况！ 1.请求1执行完成15ms后请求2才到达，此时立即执行请求2，请求3在请求2执行完成5ms后到达， 理论上要sleep 5ms后才能执行请求3，但是如此的话总耗时就为15+10=25ms，理想状态应该是20ms才可以， 所以引入【将之前所有多余的耗时累计起来，在后续做抵消使用，该值可正可负， 正：前面等待时间过长超过preRequest，负：前面等待时间不足preRequest】 </description>
    </item>
    
  </channel>
</rss>
